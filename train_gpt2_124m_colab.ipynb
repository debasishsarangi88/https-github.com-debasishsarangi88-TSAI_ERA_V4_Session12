{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 124M Training on Shakespeare Dataset\n",
        "\n",
        "This notebook trains a decoder-only 124M parameter GPT-2 model on Shakespeare text.\n",
        "\n",
        "**Target**: Train until loss < 0.1\n",
        "\n",
        "**Hardware Recommendation**: GPU (T4 or better)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch tiktoken transformers matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import json\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Input Data\n",
        "\n",
        "Upload your `input.txt` file using the file upload button or use the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload input.txt file\n",
        "print(\"Please upload your input.txt file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify the file exists\n",
        "if 'input.txt' in uploaded:\n",
        "    print(\"âœ“ input.txt uploaded successfully!\")\n",
        "    with open('input.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(f\"File size: {len(text):,} characters\")\n",
        "else:\n",
        "    print(\"âŒ Please upload input.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\n",
        "            \"bias\", \n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024  # max sequence length\n",
        "    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "    n_layer: int = 12  # number of layers\n",
        "    n_head: int = 12  # number of heads\n",
        "    n_embd: int = 768  # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        \n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        \n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def get_num_params(self):\n",
        "        \"\"\"Return the number of parameters in the model\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "# Print model info\n",
        "config = GPTConfig()\n",
        "temp_model = GPT(config)\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"  Layers: {config.n_layer}\")\n",
        "print(f\"  Heads: {config.n_head}\")\n",
        "print(f\"  Embedding dim: {config.n_embd}\")\n",
        "print(f\"  Total parameters: {temp_model.get_num_params():,}\")\n",
        "del temp_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, split='train'):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # Load tokens from disk\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'Loaded {len(self.tokens):,} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T):,} batches')\n",
        "\n",
        "        # State\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)  # inputs\n",
        "        y = (buf[1:]).view(B, T)   # targets\n",
        "        \n",
        "        # Advance position\n",
        "        self.current_position += B * T\n",
        "        \n",
        "        # Reset if out of bounds\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        \n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 16  # Increased from 4 for better training\n",
        "SEQUENCE_LENGTH = 128  # Increased from 32\n",
        "LEARNING_RATE = 3e-4\n",
        "MAX_STEPS = 10000  # Will stop early if target loss is reached\n",
        "TARGET_LOSS = 0.0999999  # Target loss threshold\n",
        "EVAL_INTERVAL = 50  # Evaluate every N steps\n",
        "SAVE_INTERVAL = 500  # Save checkpoint every N steps\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Sequence length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max steps: {MAX_STEPS}\")\n",
        "print(f\"  Target loss: {TARGET_LOSS}\")\n",
        "print(f\"  Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Initialize model\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "print(f\"Model initialized with {model.get_num_params():,} parameters\")\n",
        "\n",
        "# Initialize data loader\n",
        "train_loader = DataLoaderLite(B=BATCH_SIZE, T=SEQUENCE_LENGTH)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\nâœ“ Model and optimizer ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop with Comprehensive Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = []\n",
        "start_time = time.time()\n",
        "best_loss = float('inf')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    # Training step\n",
        "    step_start_time = time.time()\n",
        "    \n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    step_time = time.time() - step_start_time\n",
        "    \n",
        "    # Log training metrics\n",
        "    log_entry = {\n",
        "        'step': step,\n",
        "        'loss': loss.item(),\n",
        "        'time': step_time,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    training_logs.append(log_entry)\n",
        "    \n",
        "    # Print progress\n",
        "    if step % EVAL_INTERVAL == 0 or step == MAX_STEPS - 1:\n",
        "        elapsed = time.time() - start_time\n",
        "        tokens_processed = step * BATCH_SIZE * SEQUENCE_LENGTH\n",
        "        tokens_per_sec = tokens_processed / elapsed if elapsed > 0 else 0\n",
        "        \n",
        "        print(f\"Step {step:5d} | Loss: {loss.item():.6f} | \"\n",
        "              f\"Time: {step_time:.3f}s | \"\n",
        "              f\"Tokens/s: {tokens_per_sec:.0f} | \"\n",
        "              f\"Elapsed: {elapsed:.0f}s\")\n",
        "    \n",
        "    # Save checkpoint (periodic - includes optimizer for resuming training)\n",
        "    if step > 0 and step % SAVE_INTERVAL == 0:\n",
        "        checkpoint_path = f'checkpoints/model_step_{step}.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Needed to resume training\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"  ðŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "    \n",
        "    # Check if target loss reached\n",
        "    if loss.item() < TARGET_LOSS:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸŽ‰ TARGET LOSS REACHED! Loss: {loss.item():.6f} < {TARGET_LOSS}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Save final model (deployment version - NO optimizer state)\n",
        "        # This creates a smaller file (~500MB vs ~1.4GB)\n",
        "        final_checkpoint_path = 'checkpoints/model_final.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, final_checkpoint_path)\n",
        "        print(f\"âœ“ Final model saved: {final_checkpoint_path} (~500MB, deployment-ready)\")\n",
        "        \n",
        "        # Also save full version with optimizer (for potential fine-tuning)\n",
        "        full_checkpoint_path = 'checkpoints/model_final_full.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, full_checkpoint_path)\n",
        "        print(f\"âœ“ Full checkpoint saved: {full_checkpoint_path} (~1.4GB, for fine-tuning)\")\n",
        "        break\n",
        "    \n",
        "    # Update best loss (save deployment version)\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        \n",
        "        # Save deployment version (without optimizer - smaller file)\n",
        "        best_checkpoint_path = 'checkpoints/model_best.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, best_checkpoint_path)\n",
        "        print(f\"  â­ Best model updated: {best_checkpoint_path} (loss: {best_loss:.6f})\")\n",
        "\n",
        "# Save training logs\n",
        "log_file = f'logs/training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(log_file, 'w') as f:\n",
        "    json.dump(training_logs, f, indent=2)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "print(f\"Final loss: {loss.item():.6f}\")\n",
        "print(f\"Best loss: {best_loss:.6f}\")\n",
        "print(f\"Total steps: {step + 1}\")\n",
        "print(f\"Training log saved: {log_file}\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Checkpoint Files Explained\n",
        "\n",
        "### File Sizes & Purpose\n",
        "\n",
        "The training saves different types of checkpoints:\n",
        "\n",
        "| Checkpoint | Size | Contains | Purpose |\n",
        "|------------|------|----------|---------|\n",
        "| **model_final.pt** | ~500 MB | Model weights only | âœ… **Use this for deployment** (Hugging Face, production) |\n",
        "| **model_final_full.pt** | ~1.4 GB | Model + Optimizer | For fine-tuning or resuming training |\n",
        "| **model_step_X.pt** | ~1.4 GB | Model + Optimizer | Periodic checkpoints during training |\n",
        "| **model_best.pt** | ~500 MB | Model weights only | Best loss checkpoint (deployment-ready) |\n",
        "\n",
        "### Why the Size Difference?\n",
        "\n",
        "- **Model weights**: 124M parameters Ã— 4 bytes = ~500 MB\n",
        "- **Optimizer state** (Adam): 124M Ã— 2 Ã— 4 bytes = ~900 MB (momentum + variance)\n",
        "- **Total with optimizer**: ~1.4 GB\n",
        "\n",
        "### Which File to Use?\n",
        "\n",
        "âœ… **For Hugging Face deployment**: Use `model_final.pt` (~500 MB)\n",
        "- Faster upload\n",
        "- Lower storage costs  \n",
        "- Perfect for inference\n",
        "\n",
        "ðŸ“š **For future training**: Keep `model_final_full.pt` (~1.4 GB)\n",
        "- Can resume training\n",
        "- Can fine-tune on new data\n",
        "- Includes optimizer state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training loss\n",
        "steps = [log['step'] for log in training_logs]\n",
        "losses = [log['loss'] for log in training_logs]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(steps, losses, linewidth=0.5, alpha=0.6, label='Training Loss')\n",
        "plt.axhline(y=TARGET_LOSS, color='r', linestyle='--', label=f'Target Loss ({TARGET_LOSS})')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('logs/training_loss.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Loss plot saved: logs/training_loss.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample text\n",
        "model.eval()\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "num_return_sequences = 3\n",
        "max_length = 100\n",
        "prompt = \"First Citizen:\"\n",
        "\n",
        "print(\"\\nGenerating sample text...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tokens = enc.encode(prompt)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(decoded)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model Weights\n",
        "\n",
        "Download the trained model weights to use in Hugging Face Spaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create a zip file with all checkpoints and logs\n",
        "shutil.make_archive('trained_model', 'zip', '.', 'checkpoints')\n",
        "shutil.make_archive('training_logs', 'zip', '.', 'logs')\n",
        "\n",
        "print(\"Downloading model weights and logs...\")\n",
        "files.download('trained_model.zip')\n",
        "files.download('training_logs.zip')\n",
        "print(\"\\nâœ“ Downloads complete!\")\n",
        "print(\"\\nYou can now use these weights in the Hugging Face Spaces app.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
