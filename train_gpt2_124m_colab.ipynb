{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 124M Training on Shakespeare Dataset\n",
        "\n",
        "This notebook trains a decoder-only 124M parameter GPT-2 model on Shakespeare text.\n",
        "\n",
        "**Target**: Train until loss < 0.1\n",
        "\n",
        "**Hardware Recommendation**: GPU (T4 or better)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch tiktoken transformers matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import json\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Input Data\n",
        "\n",
        "Upload your `input.txt` file using the file upload button or use the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload input.txt file\n",
        "print(\"Please upload your input.txt file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify the file exists\n",
        "if 'input.txt' in uploaded:\n",
        "    print(\"âœ“ input.txt uploaded successfully!\")\n",
        "    with open('input.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(f\"File size: {len(text):,} characters\")\n",
        "else:\n",
        "    print(\"âŒ Please upload input.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\n",
        "            \"bias\", \n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024  # max sequence length\n",
        "    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "    n_layer: int = 12  # number of layers\n",
        "    n_head: int = 12  # number of heads\n",
        "    n_embd: int = 768  # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        \n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        \n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def get_num_params(self):\n",
        "        \"\"\"Return the number of parameters in the model\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "# Print model info\n",
        "config = GPTConfig()\n",
        "temp_model = GPT(config)\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"  Layers: {config.n_layer}\")\n",
        "print(f\"  Heads: {config.n_head}\")\n",
        "print(f\"  Embedding dim: {config.n_embd}\")\n",
        "print(f\"  Total parameters: {temp_model.get_num_params():,}\")\n",
        "del temp_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, split='train'):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # Load tokens from disk\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'Loaded {len(self.tokens):,} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T):,} batches')\n",
        "\n",
        "        # State\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)  # inputs\n",
        "        y = (buf[1:]).view(B, T)   # targets\n",
        "        \n",
        "        # Advance position\n",
        "        self.current_position += B * T\n",
        "        \n",
        "        # Reset if out of bounds\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        \n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 16  # Increased from 4 for better training\n",
        "SEQUENCE_LENGTH = 128  # Increased from 32\n",
        "LEARNING_RATE = 3e-4\n",
        "MAX_STEPS = 10000  # Will stop early if target loss is reached\n",
        "TARGET_LOSS = 0.0999999  # Target loss threshold\n",
        "EVAL_INTERVAL = 50  # Evaluate every N steps\n",
        "SAVE_INTERVAL = 500  # Save checkpoint every N steps\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Sequence length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max steps: {MAX_STEPS}\")\n",
        "print(f\"  Target loss: {TARGET_LOSS}\")\n",
        "print(f\"  Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Initialize model\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "print(f\"Model initialized with {model.get_num_params():,} parameters\")\n",
        "\n",
        "# Initialize data loader\n",
        "train_loader = DataLoaderLite(B=BATCH_SIZE, T=SEQUENCE_LENGTH)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\nâœ“ Model and optimizer ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop with Comprehensive Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = []\n",
        "start_time = time.time()\n",
        "best_loss = float('inf')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    # Training step\n",
        "    step_start_time = time.time()\n",
        "    \n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    step_time = time.time() - step_start_time\n",
        "    \n",
        "    # Log training metrics\n",
        "    log_entry = {\n",
        "        'step': step,\n",
        "        'loss': loss.item(),\n",
        "        'time': step_time,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    training_logs.append(log_entry)\n",
        "    \n",
        "    # Print progress\n",
        "    if step % EVAL_INTERVAL == 0 or step == MAX_STEPS - 1:\n",
        "        elapsed = time.time() - start_time\n",
        "        tokens_processed = step * BATCH_SIZE * SEQUENCE_LENGTH\n",
        "        tokens_per_sec = tokens_processed / elapsed if elapsed > 0 else 0\n",
        "        \n",
        "        print(f\"Step {step:5d} | Loss: {loss.item():.6f} | \"\n",
        "              f\"Time: {step_time:.3f}s | \"\n",
        "              f\"Tokens/s: {tokens_per_sec:.0f} | \"\n",
        "              f\"Elapsed: {elapsed:.0f}s\")\n",
        "    \n",
        "    # Save checkpoint (periodic - includes optimizer for resuming training)\n",
        "    if step > 0 and step % SAVE_INTERVAL == 0:\n",
        "        checkpoint_path = f'checkpoints/model_step_{step}.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Needed to resume training\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"  ðŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "    \n",
        "    # Check if target loss reached\n",
        "    if loss.item() < TARGET_LOSS:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸŽ‰ TARGET LOSS REACHED! Loss: {loss.item():.6f} < {TARGET_LOSS}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Save final model (deployment version - NO optimizer state)\n",
        "        # This creates a smaller file (~500MB vs ~1.4GB)\n",
        "        final_checkpoint_path = 'checkpoints/model_final.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, final_checkpoint_path)\n",
        "        print(f\"âœ“ Final model saved: {final_checkpoint_path} (~500MB, deployment-ready)\")\n",
        "        \n",
        "        # Also save full version with optimizer (for potential fine-tuning)\n",
        "        full_checkpoint_path = 'checkpoints/model_final_full.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, full_checkpoint_path)\n",
        "        print(f\"âœ“ Full checkpoint saved: {full_checkpoint_path} (~1.4GB, for fine-tuning)\")\n",
        "        break\n",
        "    \n",
        "    # Update best loss (save deployment version)\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        \n",
        "        # Save deployment version (without optimizer - smaller file)\n",
        "        best_checkpoint_path = 'checkpoints/model_best.pt'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            'config': config.__dict__,\n",
        "        }, best_checkpoint_path)\n",
        "        print(f\"  â­ Best model updated: {best_checkpoint_path} (loss: {best_loss:.6f})\")\n",
        "\n",
        "# Save training logs\n",
        "log_file = f'logs/training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(log_file, 'w') as f:\n",
        "    json.dump(training_logs, f, indent=2)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "print(f\"Final loss: {loss.item():.6f}\")\n",
        "print(f\"Best loss: {best_loss:.6f}\")\n",
        "print(f\"Total steps: {step + 1}\")\n",
        "print(f\"Training log saved: {log_file}\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Checkpoint Files Explained\n",
        "\n",
        "### File Sizes & Purpose\n",
        "\n",
        "The training saves different types of checkpoints:\n",
        "\n",
        "| Checkpoint | Size | Contains | Purpose |\n",
        "|------------|------|----------|---------|\n",
        "| **model_final.pt** | ~500 MB | Model weights only | âœ… **Use this for deployment** (Hugging Face, production) |\n",
        "| **model_final_full.pt** | ~1.4 GB | Model + Optimizer | For fine-tuning or resuming training |\n",
        "| **model_step_X.pt** | ~1.4 GB | Model + Optimizer | Periodic checkpoints during training |\n",
        "| **model_best.pt** | ~500 MB | Model weights only | Best loss checkpoint (deployment-ready) |\n",
        "\n",
        "### Why the Size Difference?\n",
        "\n",
        "- **Model weights**: 124M parameters Ã— 4 bytes = ~500 MB\n",
        "- **Optimizer state** (Adam): 124M Ã— 2 Ã— 4 bytes = ~900 MB (momentum + variance)\n",
        "- **Total with optimizer**: ~1.4 GB\n",
        "\n",
        "### Which File to Use?\n",
        "\n",
        "âœ… **For Hugging Face deployment**: Use `model_final.pt` (~500 MB)\n",
        "- Faster upload\n",
        "- Lower storage costs  \n",
        "- Perfect for inference\n",
        "\n",
        "ðŸ“š **For future training**: Keep `model_final_full.pt` (~1.4 GB)\n",
        "- Can resume training\n",
        "- Can fine-tune on new data\n",
        "- Includes optimizer state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training loss\n",
        "steps = [log['step'] for log in training_logs]\n",
        "losses = [log['loss'] for log in training_logs]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(steps, losses, linewidth=0.5, alpha=0.6, label='Training Loss')\n",
        "plt.axhline(y=TARGET_LOSS, color='r', linestyle='--', label=f'Target Loss ({TARGET_LOSS})')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('logs/training_loss.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Loss plot saved: logs/training_loss.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample text\n",
        "model.eval()\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "num_return_sequences = 3\n",
        "max_length = 100\n",
        "prompt = \"First Citizen:\"\n",
        "\n",
        "print(\"\\nGenerating sample text...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tokens = enc.encode(prompt)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(decoded)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¥ Download Model Weights & Files\n",
        "\n",
        "This will download your trained model files directly to your computer.\n",
        "\n",
        "### Files that will be downloaded:\n",
        "\n",
        "1. **`model_final.pt`** (~500 MB) âœ…\n",
        "   - Deployment-ready checkpoint\n",
        "   - **Use this file for Hugging Face Spaces**\n",
        "   - Model weights only (no optimizer)\n",
        "\n",
        "2. **`model_best.pt`** (~500 MB)\n",
        "   - Best checkpoint from training (lowest loss)\n",
        "   - Alternative deployment option\n",
        "\n",
        "3. **`training_loss.png`**\n",
        "   - Loss curve visualization\n",
        "   - Shows training progress\n",
        "\n",
        "4. **Training log (JSON)**\n",
        "   - Complete training metrics\n",
        "   - Step-by-step loss values\n",
        "\n",
        "### Note:\n",
        "- Downloads may take a few minutes for large files\n",
        "- Files will be saved to your browser's Downloads folder\n",
        "- If download fails, use Files panel (left sidebar) â†’ right-click â†’ Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“¥ DOWNLOADING MODEL FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show file sizes\n",
        "print(\"\\nAvailable files:\")\n",
        "if os.path.exists('checkpoints/model_final.pt'):\n",
        "    final_size = os.path.getsize('checkpoints/model_final.pt') / (1024**3)\n",
        "    print(f\"  âœ… model_final.pt: {final_size:.2f} GB (deployment-ready)\")\n",
        "    \n",
        "if os.path.exists('checkpoints/model_final_full.pt'):\n",
        "    full_size = os.path.getsize('checkpoints/model_final_full.pt') / (1024**3)\n",
        "    print(f\"  ðŸ“š model_final_full.pt: {full_size:.2f} GB (for fine-tuning)\")\n",
        "\n",
        "if os.path.exists('checkpoints/model_best.pt'):\n",
        "    best_size = os.path.getsize('checkpoints/model_best.pt') / (1024**3)\n",
        "    print(f\"  â­ model_best.pt: {best_size:.2f} GB (best checkpoint)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Starting downloads... (this may take a few minutes)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Download essential files\n",
        "try:\n",
        "    # Download model_final.pt (deployment version)\n",
        "    if os.path.exists('checkpoints/model_final.pt'):\n",
        "        print(\"\\nðŸ“¥ Downloading model_final.pt (~500 MB)...\")\n",
        "        print(\"   This is the main file you need for Hugging Face deployment\")\n",
        "        files.download('checkpoints/model_final.pt')\n",
        "        print(\"   âœ“ model_final.pt downloaded\")\n",
        "    \n",
        "    # Download training loss plot\n",
        "    if os.path.exists('logs/training_loss.png'):\n",
        "        print(\"\\nðŸ“¥ Downloading training_loss.png...\")\n",
        "        files.download('logs/training_loss.png')\n",
        "        print(\"   âœ“ training_loss.png downloaded\")\n",
        "    \n",
        "    # Download latest training log\n",
        "    log_files = [f for f in os.listdir('logs') if f.endswith('.json')]\n",
        "    if log_files:\n",
        "        latest_log = sorted(log_files)[-1]\n",
        "        print(f\"\\nðŸ“¥ Downloading {latest_log}...\")\n",
        "        files.download(f'logs/{latest_log}')\n",
        "        print(f\"   âœ“ {latest_log} downloaded\")\n",
        "    \n",
        "    # Optionally download model_best.pt\n",
        "    if os.path.exists('checkpoints/model_best.pt'):\n",
        "        print(\"\\nðŸ“¥ Downloading model_best.pt (best checkpoint)...\")\n",
        "        files.download('checkpoints/model_best.pt')\n",
        "        print(\"   âœ“ model_best.pt downloaded\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"âœ… ALL DOWNLOADS COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nðŸ“¦ Downloaded files:\")\n",
        "    print(\"  âœ“ model_final.pt - Use this for Hugging Face deployment\")\n",
        "    print(\"  âœ“ model_best.pt - Alternative checkpoint\")\n",
        "    print(\"  âœ“ training_loss.png - Visualization\")\n",
        "    print(\"  âœ“ Training log - Complete metrics (JSON)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Download error: {e}\")\n",
        "    print(\"\\nAlternative: Use the Files panel on the left sidebar\")\n",
        "    print(\"  1. Click the folder icon on the left\")\n",
        "    print(\"  2. Navigate to checkpoints/\")\n",
        "    print(\"  3. Right-click model_final.pt â†’ Download\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸ“Œ NEXT STEPS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. Locate downloaded files in your Downloads folder\")\n",
        "print(\"2. Upload model_final.pt to your Hugging Face Space\")\n",
        "print(\"3. Your model is ready to deploy!\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
